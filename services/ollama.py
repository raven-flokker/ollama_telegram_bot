import requests
from config import OLLAMA_URL, MODEL_NAME
from services.prompts import SYSTEM_PROMPT
from services.memory import add_message, get_context
import logging
import httpx

logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç –æ–¥–∏–Ω —Ä–∞–∑.
# –ü—Ä–∏–±–ª—É–¥—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–≥–¥–∞ –±–æ—Ç –≤–∏—Å–∏—Ç –æ—Ç –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
client = httpx.AsyncClient(
    base_url=OLLAMA_URL,
    timeout=httpx.Timeout(60.0)
)


async def ask_llm(user_id: int, prompt: str) -> str:  # —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ user_id
    logging.info(f"[Ollama] –ó–∞–ø—Ä–æ—Å –æ—Ç user_id={user_id}: {prompt}")
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        add_message(user_id, "user", prompt)

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = get_context(user_id)

        # response = requests.post(
        response = await client.post(
            # f"{OLLAMA_URL}/api/generate",
            "/api/chat",  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ chat –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            json={
                "model": MODEL_NAME,
                "messages": context,
                # "prompt": prompt,
                # "system": SYSTEM_PROMPT,
                "stream": False,
                "think": False,  # –≠—Ç–æ —Ç–æ—á–Ω–æ –≤—ã–∫–ª—é—á–∞–µ—Ç –º—ã—à–ª–µ–Ω–∏–µ (–≤—ã–≤–æ–¥ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è)
                "role": "assistant",
                "options": {
                    "temperature": 0.8,  # –ë–∞–ª–∞–Ω—Å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ 0.8 –¥–µ—Ñ–æ–ª—Ç
                    "top_p": 0.9,
                    "repeat_penalty": 1.1,
                    "num_predict": 1000,  # –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
                    "nothink": True,  # –°–∫–æ—Ä–µ–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                    "think": False  # –°–∫–æ—Ä–µ–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç

                }
            },
            # timeout=30
        )
        response.raise_for_status()
        # return response.json()["response"].strip()
        answer = response.json()["message"]["content"].strip()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç
        add_message(user_id, "assistant", answer)
        logging.info(f"[Ollama] –û—Ç–≤–µ—Ç: {answer[:50]}...")
        return answer
    except requests.exceptions.RequestException as e:
        print(f"[Ollama] Error: {e}")
        logging.error(f"[Ollama] –û—à–∏–±–∫–∞: {e}")
        # return "–ö–∞–∂–µ—Ç—Å—è –∫–æ—Ç–∏–∫ —É–º–µ—Ä. –ú—ã —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–¥ –µ–≥–æ –≤–æ—Å–∫—Ä–µ—à–µ–Ω–∏–µ–º."
        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π user-—Å–æ–æ–±—â–µ–Ω–∏–µ, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        try:
            ctx = get_context(user_id)
            if len(ctx) > 1 and ctx[-1]["role"] == "user":
                ctx.pop()
                from services.memory import save_context
                save_context(user_id, ctx)
        except:
            pass
        return "–ú—è—É. –Ø –ø–æ—Ç–µ—Ä—è–ª –º—ã—Å–ª—å. –ü–æ–≤—Ç–æ—Ä–∏? ü§î"
